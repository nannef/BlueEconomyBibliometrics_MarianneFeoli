---
title: "BE_Biliometrics2023"
author: "Marianne Feoli Martinez"
format: html
editor: visual
---

# Blue Economy Bibliometrics

The following code can be used to run the bibliometric and metadata analysis used for the article: **Navigating the Changing Tides: Exploring Shifting Research Trends in the Blue Economy**

The article aimed to shed light on the prevailing perceptions of this terminology among the general public and scientific community by examining the industries driving research in this field and any associated knowledge gaps and potential biases that may have influenced these perception patterns, by unveiling the evolution of the concept through the growing body of published literature and identifying potential factors contributing to its criticisms. By identifying key proponents and geographical, economic, and research gaps or biases, this research aimed to assist advocates of the Blue Economy in addressing the current influx of negativity, ensuring the movement remains a unified global endeavor.

## Part 1. Setup the R environment

You might need to install some of the libraries.

Citations for the libraries that were used the most can be found in the article's references.

```{r}
#| include: false
rm(list = ls())
library(bibliometrix)	#for bibliometrics analysis
library(rAltmetric) #for altimetrics analysis
library(here) #to set the working directory
#The following libraries were used for developing graphs and tidying up data
library(tidyverse)
library(stringr)
library(ggplot2)
library(dplyr)
library(tidyr)
library(splitstackshape)
library(forcats)
library(maps)
library(ggspatial)
library(viridis)
library(pheatmap)
library(RColorBrewer)
library(dendextend)
library(DT)
library(reshape2)
library(scales)
getwd() #to check that we are in the right working directory
```

## Part 2. Call in the data

This part shows the code used to tidy up the raw datasets exported from Scopus and Rayyan, and calls on the final, tidy datasets.

Section 2.1 and 2.2 can be skipped and the code will still work (since we created new csv files with the tidy sets in part 2.3). That said, ***if you run part 2.1 and 2.2, it is not necessary to run part 2.3***.

### 2.1 Tidying up

Because we had a long list of articles, we had to download them in blocks. We converted them to bibliometric dataframes to then merge together. Keep in mind, we created a new dataframe that contains all to avoid needing to run this part. You can find it in section 2.2. That said, you can skip this first block.

```{r}
#| include: false
bib1 <- convert2df(file = here("Articles/scopus (1).bib"), dbsource = "scopus", format = "bibtex")
bib2 <- convert2df(file = here("Articles/scopus (2).bib"), dbsource = "scopus", format = "bibtex")
bib3 <- convert2df(file = here("Articles/scopus (3).bib"), dbsource = "scopus", format = "bibtex")
bib4 <- convert2df(file = here("Articles/scopus (4).bib"), dbsource = "scopus", format = "bibtex")
bib5 <- convert2df(file = here("Articles/scopus (5).bib"), dbsource = "scopus", format = "bibtex")
bib6 <- convert2df(file = here("Articles/scopus (6).bib"), dbsource = "scopus", format = "bibtex")
bib7 <- convert2df(file = here("Articles/scopus (7).bib"), dbsource = "scopus", format = "bibtex")
bib8 <- convert2df(file = here("Articles/scopus (8).bib"), dbsource = "scopus", format = "bibtex")
bib9 <- convert2df(file = here("Articles/scopus (9).bib"), dbsource = "scopus", format = "bibtex")
#merged
bib <- bind_rows(bib1, bib2, bib3, bib4, bib5, bib6, bib7, bib8, bib9)
names(bib) #to see column names
dim(bib) #to chech number of records (703)
write.csv(bib, "Processed data frames/bib_as_df.csv", row.names = FALSE) #to save this data frame as a csv file
```

Similarly, the Rayyan dataset had a lot of tidying up to do, so I will share the code here, but this step can be skipped when calling upon the tidy dataset in part 2.2.

```{r}
#| include: false
#First we call the exported Rayyan results
bibmeta <- read.csv(file = here("Rayyan results/articles.csv"), header = TRUE)
#names(bibmeta) #the columns we are interested are column 4 (article_id) and 5 (key)
#The columns Included, Classification and View were created in the Excel spreadsheet using =IF/ELSE formulas. E.g. =if(Q2=OR("*Marianne => Included*","*Xiaoyu => Included*");"Yes";"No")

#There were several inclusion/exclusion conflicts in the dataset that couldn't be settled, so we will remove the conflicts by only using the criteria submitted by Xiaoyu and Marianne. 
bibmeta <- bibmeta[bibmeta$Included != "No", ] #this will exclude the articles that we excluded

#Now we find the sectors
#first remove unwanted columns (just because it is easier to work with)
bib_sector <-subset(bibmeta, select = -c(title, year, month, day, journal, issn, volume, issue, pages, authors, url, doi, language, publisher, location, abstract, keywords, pubmed_id, pmc_id, Included, Classification, View))
#Change commas for spaces 
bib_sector <- bib_sector %>%
 mutate(notes = str_replace_all(notes, ",", " "))
#Remove unwanted words from the notes column
exclusion_words <- c("Export", "Date:", "Cited", "By:", "RAYYAN-INCLUSION:", "Marianne", "Included", "RAYYAN-LABELS:","neutral", "positive", "negative", "Tier", "(BE)", "(Applications)","Tier", "Tier1(BE)", "Tier2(Applications)", "Tier1", "1(BE)", "Tier2", "2(Applications)", "Xiaoyu", "Basil", "Excluded", "January", "February", "March", "April","May", "June", "July", "August", "September", "October", "November", "December","\\|", "\\{", "\\}", "=>", "be")
bib_sector <- bib_sector %>%
  mutate(notes = str_replace_all(notes, paste(exclusion_words, collapse = "|"), "")) %>%
  mutate(notes = str_replace_all(notes, "\\b\\d+\\b", "")) %>%
  mutate(notes = str_replace_all(notes, "[;()|{}=>]+", "")) %>%
  mutate(notes = trimws(notes)) %>%
  mutate(notes = str_replace_all(notes, "Oil and gas", "Oil&Gas")) %>%
  mutate(notes = str_replace_all(notes, "tourism", "Tourism")) %>%
  mutate(notes = str_replace_all(notes, "Desalinization", "Others")) %>%
  mutate(notes = str_replace_all(notes, "Blue carbon", "Others")) %>%
  mutate(notes = str_replace_all(notes, "Biofuels", "Energy")) %>%
  mutate(notes = str_replace_all(notes, "agriculture", "Others"))
#Now we Split sectors into separate columns (called notes 1 and notes 2)
bib_sector <- cSplit(bib_sector, "notes", " ", direction = "wide")
bib_sector$notes_1[is.na(bib_sector$notes_1)] <- "non-targeted"
bib_sector <- subset(bib_sector, select = -c(notes_3))
#Check industries
#unique(bib_sector$notes_1) #sector column one
#unique(bib_sector$notes_2) #sector column two

#Because they are too many, now we substitute words in order to group together (to make things simpler)
bib_sector <- bib_sector %>%
  mutate(notes_1 = str_replace_all(notes_1, "\\b(fisheries|aquaculture)\\b", "Fisheries&Aquaculture")) %>%
  mutate(notes_1 = str_replace_all(notes_1, "\\b(Oil&Gas|Mining|Mining&Oil&Gas)\\b", "Mining&Oil&Gas")) %>%
  mutate(notes_1 = str_replace_all(notes_1, "\\b(Security|Shipping|Maritime)\\b", "Shipping&Maritime")) %>%
  mutate(notes_1 = str_replace_all(notes_1, "\\b(Biotechnology|pharmaceutical)\\b", "Biotechnology&Pharmaceutical")) %>%
  mutate(notes_2 = str_replace_all(notes_2, "\\b(fisheries|aquaculture)\\b", "Fisheries&Aquaculture")) %>%
  mutate(notes_2 = str_replace_all(notes_2, "\\b(Oil&Gas|Mining|Mining&Oil&Gas)\\b", "Mining&Oil&Gas")) %>%
  mutate(notes_2 = str_replace_all(notes_2, "\\b(Security|Shipping|Maritime)\\b", "Shipping&Maritime")) %>%
  mutate(notes_2 = str_replace_all(notes_2, "\\b(Biotechnology|pharmaceutical)\\b", "Biotechnology&Pharmaceutical"))

#Now we group when the same sector repeated and merge them
bib_sector$notes_2 <- ifelse(bib_sector$notes_1 == bib_sector$notes_2, "", bib_sector$notes_2)
#check industries again to see if it worked
#unique(bib_sector$notes_1) 
#unique(bib_sector$notes_2) 
#sum(!is.na(bib_sector$notes_2) & bib_sector$notes_2 != "") #count how many inputs in column 2

#Now we will merge bib_sector with bibmeta to tidy up, but first we will create an alternative dataset where each sector is a new line (for sector analyses)
#first I will change the new column names
colnames(bib_sector)[colnames(bib_sector) == "notes_1"] <- "Sector"
colnames(bib_sector)[colnames(bib_sector) == "notes_2"] <- "OtherSectors"

#now I create the new sets
bibmetaSEP <- merge(bibmeta, bib_sector, by = 'key') #sectors divided in two columns
#Sectors listed as new rows (duplicated article)
bibmetaEXTRA <- bib_sector %>%
  filter(OtherSectors != "") %>%
  mutate(Sector = strsplit(as.character(OtherSectors), ";")) %>%
  unnest(Sector) %>%
  select(-OtherSectors) %>%
  bind_rows(subset(bib_sector, select = -c(OtherSectors))) %>%
  merge(bibmeta, ., by = 'key')

names(bibmetaEXTRA)
names(bibmetaSEP)

#Saving to avoid having to go through this again
write.csv(bibmetaEXTRA, file = here("Processed data frames/meta_asrows.csv"), row.names = FALSE)
write.csv(bibmetaSEP, file = here("Processed data frames/meta_ascolumns.csv"), row.names = FALSE) 
```

An extra step here is extracting the altimetrics data from the datasets, which we added in this section so that you don't have to process it (again we saved the final dataframe and can be called upon in section 2.2). Note you need internet to get this information.

```{r}
#| include: false
#First we have to make sure we have the dataframe bib
bib <- read.csv(file = here("Processed data frames/bib_as_df.csv"), header=TRUE) #run this line if the bibliometrics part was not runned 

#define getAltmetrics() function 
getAltmetrics <- function(doi = NULL,
                          foptions = list(),
                           ...) {
    if (!is.null(doi)) doi <- stringr::str_c("doi/", doi)
    identifiers <- purrr::compact(list(doi))
    if (!is.null(identifiers)) {
      ids <- identifiers[[1]]
    }
    base_url <- "http://api.altmetric.com/v1/"
    request <- httr::GET(paste0(base_url, ids))
    results <-
      jsonlite::fromJSON(httr::content(request, as = "text"), flatten = TRUE)
    results <- rlist::list.flatten(results)
    class(results) <- "altmetric"
    results
}

#define helper function
altmetric_df <- function(altmetric.object) {
  df <- data.frame(t(unlist(altmetric.object)), 
                   stringsAsFactors = FALSE)
}

#Run the function for all articles from the bib dataframe, using their DOI as a vector:
dois <- bib$DI #get all dois
#run custom function and try to catch errors if not found (articles which have 0 Altmetric score are not in the database)
Altmetrics.results <- list(dois) %>% purrr::pmap( ~ tryCatch( {getAltmetrics(doi = .x) %>% altmetric_df()},
                                             error = function(e) {
                                             cat(paste0("•NOT_FOUND• ", .x, "\n"))
                                             }))
length(Altmetrics.results) #to check the number of articles (100)
#table(sapply(Altmetrics.results, is.null)) #some are NULL - no altmetrics data
Altmetrics.results[sapply(Altmetrics.results, is.null)] <- NULL #to remove empty list elements

#now convert list of altmetrics results to a cleaner data frame - it drops empty rows
Altmetrics <- data.table::rbindlist(Altmetrics.results, fill = TRUE)
#dim(Altmetrics) #338 rows by 181 columns
Altmetrics$score <- as.numeric(Altmetrics$score) #make numeric
Altmetrics$published_date <- as.POSIXct(as.numeric(gsub("NA        ", NA, Altmetrics$published_on)), origin = "1970-01-01")  #change "NA        " to NA and convert to numeric then convert to human-readable data and time
Altmetrics$year <- as.numeric(format(Altmetrics$published_date, format="%Y")) #extract year

readr::write_csv(Altmetrics, file = here('Processed data frames',"Altmetrics.results_df.csv")) #save the processed dataframe 
#print(here("Altmetrics.results_df.csv"))
```

### 2.2 Joining the datasets

```{r}
#| include: false
#join the Rayyan screening with the bibliometrics
#first we will only keep the tags
meta_tags <-subset(bibmetaSEP, select = -c(key, title, year, month, day, journal, issn, volume, issue, pages, authors, url, language, publisher, location, abstract, notes, keywords, pubmed_id, pmc_id))
#change doi name column to DI
colnames(meta_tags)[colnames(meta_tags) == "doi"] <- "DI"
ALLDATA_SEP <- merge(meta_tags, bib, by = 'DI') 
#dim(ALLDATA_SEP) #668 observations (668 articles have DOIs)

#Now we will do the same but for sectors separated in rows (not column)
meta_tagsEXTRA <-subset(bibmetaEXTRA, select = -c(key, title, year, month, day, journal, issn, volume, issue, pages, authors, url, language, publisher, location, abstract, notes, keywords, pubmed_id, pmc_id))
#change doi name column to DI
colnames(meta_tagsEXTRA)[colnames(meta_tagsEXTRA) == "doi"] <- "DI"
ALLDATA_EXTRA <- merge(meta_tagsEXTRA, bib, by = 'DI') 
#dim(ALLDATA_EXTRA) #690 observations (668 articles have DOIs, some more than one sector)

#now we will join with the altimetrics set (but we won't use this one for analyses)
#first we will only keep the columns of interest
names(Altmetrics)
alt_tags <- Altmetrics[, c("doi", "score", "cited_by_policies_count", "cited_by_msm_count", "cited_by_feeds_count", "cited_by_tweeters_count", "cited_by_fbwalls_count", "cited_by_rdts_count", "context.journal.pct")]

#change doi name column to DI
colnames(alt_tags)[colnames(alt_tags) == "doi"] <- "DI"

#final sets
ALL <- merge(alt_tags, ALLDATA_SEP, by = 'DI') #final dataset with all data (for point of view)
ALL_EXTRA <- merge(alt_tags, ALLDATA_EXTRA, by = 'DI') #final dataset will extra data rows (for sector analysis)

#saving to avoid having to go through this again
readr::write_csv(ALLDATA_SEP, file = here('Processed data frames',"bib&meta.csv"))
readr::write_csv(ALLDATA_EXTRA, file = here('Processed data frames',"bib&metaEXTRA.csv"))
readr::write_csv(ALL, file = here('Processed data frames',"ALLtogether.csv"))
readr::write_csv(ALL_EXTRA, file = here('Processed data frames',"ALLtogether_Extra.csv"))
```

### 2.3 Calling the tidy datasets

If you didn't run section 2.1, we start by calling the bibliometrics merged dataframe, and the processed results from the Rayyan screening (we call them separetely cause we will use them separately for most analyses since the joint sets are restricted to the data -e.i. altimetrics, DOIs found)

```{r}
#| include: false
#Call the bibliometrics dataframe
bib <- read.csv(file = here("Processed data frames/bib_as_df.csv"), header=TRUE)
#names(bib) #to check columns
#table(is.na(bib$DI)) #To check missing DOI (31)

#Call the tidy rayyan screening sets
metaEXTRA <- read.csv(file = here("Processed data frames/meta_asrows.csv"), header=TRUE)
metaSEP <- read.csv(file = here("Processed data frames/meta_ascolumns.csv"), header=TRUE)

#Call altimetrics data
Altmetrics <- read.csv(file = here("Processed data frames/Altmetrics.results_df.csv"), header=TRUE)

#Call the bibliometrics and rayyan screening joined sets
bibmetaEXTRA <- read.csv(file = here("Processed data frames/bib&metaEXTRA.csv"), header=TRUE)
bibmetaSEP <- read.csv(file = here("Processed data frames/bib&meta.csv"), header=TRUE)
```

You can also call the full datasets (but we won't use them)

```{r}
ALL <- read.csv(file = here("Processed data frames/ALLtogether.csv"), header=TRUE)
ALL_EXTRA <- read.csv(file = here("Processed data frames/ALLtogether_EXTRA.csv"), header=TRUE)
```

## Part 3. General Bibliometrics

This first part will just get some general data that we are interested in using the #bibliometrics library, but then we will merge with the Rayyan set for further analyses

```{r}
#| message: false
# Preliminary descriptive analyses 
bibresults <- biblioAnalysis(bib, sep = ";")
summary(object = bibresults, k = 10, pause = TRUE) #display a series of summary tables
plot(bibresults, k = 10, pause=TRUE) #takes top 10 values from each plottable table
#to save plots into a pdf:
pdf(file = "Plots/bib_descriptive_plots.pdf", height = 8, width = 8, pointsize=10) #
plot(bibresults, k = 20, pause=FALSE) #this takes top 20 values from each plottable table
dev.off()
```

## Part 4. General Altimetrics

The most relevant results from the altimetrics analyses using #raltimetrics library

First block will analyze data and the second generates plots for better visualization

```{r}
#| message: false
#max(as.numeric(Altmetrics$score)) #highest total altmetric score (734.55)
#mean(as.numeric(Altmetrics$score)) #average total altmetric score (22.28)

##Policies
#table(Altmetrics$cited_by_policies_count, useNA = "always") #up to 4 articles cited once in policies
##table of articles with policies citations
Altmetrics %>% filter(as.numeric(cited_by_policies_count) > 0) %>% select(title, journal, authors1, year, doi, cited_by_policies_count) %>% arrange(desc(cited_by_policies_count))  %>% DT::datatable(rownames = FALSE, width = "100%", options = list(dom = 't', scrollY = '800px', pageLength = 20), caption = "Table of Altmetrics for articles cited by policies") 

##Twitter (some high numbers here)
#table(Altmetrics$cited_by_tweeters_count, useNA = "always") #up to 461 mentions per article
#sum(table(Altmetrics$cited_by_tweeters_count)) # 325 articles mentioned at least once on Tweeter
#sum(Altmetrics$cited_by_tweeters_count, na.rm = TRUE) #6067 total numbers of mentions across all articles
##table of articles with cited_by_tweeters_count >50
Altmetrics %>% filter(as.numeric(cited_by_tweeters_count) > 50) %>% select(title, journal, authors1, year, doi, cited_by_tweeters_count) %>% arrange(desc(cited_by_tweeters_count)) %>% DT::datatable(rownames = FALSE, width = "100%", options = list(dom = 't', scrollY = '800px', pageLength = 20), caption = "Table of Altmetrics for articles with Tweeter counts > 50") 

##Facebook
#table(Altmetrics$cited_by_fbwalls_count, useNA = "always") #up to 4 mentions per article
#sum(table(Altmetrics$cited_by_fbwalls_count)) #32 articles mentioned at least once
#sum(Altmetrics$cited_by_fbwalls_count, na.rm = TRUE) #44 total numbers of mentions
##table of articles with cited_by_fbwalls_count >=1
Altmetrics %>% filter(as.numeric(cited_by_fbwalls_count) >= 1) %>% select(title, journal, authors1, year, doi, cited_by_fbwalls_count) %>% arrange(desc(cited_by_fbwalls_count)) %>% DT::datatable(rownames = FALSE, width = "100%", options = list(dom = 't', scrollY = '800px', pageLength = 20), caption = "Table of Altmetrics for articles with Facebook counts > 10") 

##Number of blogs that have mentioned the publication
#table(Altmetrics$cited_by_feeds_count, useNA = "always") # up to 5 mentions
#sum(table(Altmetrics$cited_by_feeds_count)) #42 mentioned at least once
#sum(Altmetrics$cited_by_feeds_count, na.rm = TRUE) #58 total numbers of mentions
##table of articles with cited_by_feeds_count >=1
Altmetrics %>% filter(as.numeric(cited_by_feeds_count) >= 1) %>% select(title, journal, authors1, year, doi, cited_by_feeds_count) %>% arrange(desc(cited_by_feeds_count)) %>% DT::datatable(rownames = FALSE, width = "100%", options = list(dom = 't', scrollY = '800px', pageLength = 20), caption = "Table of Altmetrics for articles with blog feeds counts > 10") 

##News sources that have mentioned the publication
#table(Altmetrics$cited_by_msm_count, useNA = "always") #up to 90 
#sum(table(Altmetrics$cited_by_msm_count)) #49 mentioned at least once
#sum(Altmetrics$cited_by_msm_count, na.rm = TRUE) #377 total numbers of mentions
##table of news with cited_by_msm_count >=1
Altmetrics %>% filter(as.numeric(cited_by_msm_count) >= 1) %>% select(title, journal, authors1, year, doi, cited_by_msm_count) %>% arrange(desc(cited_by_msm_count)) %>% DT::datatable(rownames = FALSE, width = "100%", options = list(dom = 't', scrollY = '800px', pageLength = 20), caption = "Table of Altmetrics for articles with news sources counts > 10") 

##Wikipedia
#table(Altmetrics$cited_by_wikipedia_count, useNA = "always") #up to 2
#sum(table(Altmetrics$cited_by_wikipedia_count)) #7 cited at least once
#sum(Altmetrics$cited_by_wikipedia_count, na.rm = TRUE) #9 total numbers of mentions
##table of articles with cited_by_wikipedia_count >=1
Altmetrics %>% filter(as.numeric(cited_by_wikipedia_count) >= 1) %>% select(title, journal, authors1, year, doi, cited_by_wikipedia_count) %>% arrange(desc(cited_by_wikipedia_count)) %>% DT::datatable(rownames = FALSE, width = "100%", options = list(dom = 't', scrollY = '800px', pageLength = 20), caption = "Table of Altmetrics for articles with Wikipedia counts > 10") 

##The sum of all "cited_by" entries (profiles per data source)
#table(Altmetrics$cited_by_accounts_count, useNA = "always") #across all above+
##table of articles with cited_by_accounts_count >10
Altmetrics %>% filter(as.numeric(cited_by_accounts_count) > 10) %>% select(title, journal, authors1, year, doi, cited_by_accounts_count) %>% arrange(desc(cited_by_accounts_count)) %>% DT::datatable(rownames = FALSE, width = "100%", options = list(dom = 't', scrollY = '800px', pageLength = 20), caption = "Table of Altmetrics for articles with all mentions counts > 10")

#A combined table 
# Filter and select columns for each table
table_policies <- Altmetrics %>%
  filter(as.numeric(cited_by_policies_count) > 0) %>%
  select(doi, title, journal, authors1, year, cited_by_policies_count) %>%
  arrange(desc(cited_by_policies_count)) %>%
  rename(Policies = cited_by_policies_count)

table_twitter <- Altmetrics %>%
  filter(as.numeric(cited_by_tweeters_count) > 50) %>%
  select(doi, title, journal, authors1, year, cited_by_tweeters_count) %>%
  arrange(desc(cited_by_tweeters_count)) %>%
  rename(Twitter = cited_by_tweeters_count)

table_facebook <- Altmetrics %>%
  filter(as.numeric(cited_by_fbwalls_count) >= 1) %>%
  select(doi, title, journal, authors1, year, cited_by_fbwalls_count) %>%
  arrange(desc(cited_by_fbwalls_count)) %>%
  rename(Facebook = cited_by_fbwalls_count)

table_blogs <- Altmetrics %>%
  filter(as.numeric(cited_by_feeds_count) >= 1) %>%
  select(doi, title, journal, authors1, year, cited_by_feeds_count) %>%
  arrange(desc(cited_by_feeds_count)) %>%
  rename(Blogs = cited_by_feeds_count)

table_news <- Altmetrics %>%
  filter(as.numeric(cited_by_msm_count) >= 1) %>%
  select(doi, title, journal, authors1, year, cited_by_msm_count) %>%
  arrange(desc(cited_by_msm_count)) %>%
  rename(News = cited_by_msm_count)

table_wikipedia <- Altmetrics %>%
  filter(as.numeric(cited_by_wikipedia_count) >= 1) %>%
  select(doi, title, journal, authors1, year, cited_by_wikipedia_count) %>%
  arrange(desc(cited_by_wikipedia_count)) %>%
  rename(Wikipedia = cited_by_wikipedia_count)

# Combine all tables into a single table
combined_table <- bind_rows(
  "Policies" = table_policies,
  "Twitter" = table_twitter,
  "Facebook" = table_facebook,
  "Blogs" = table_blogs,
  "News sources" = table_news,
  "Wikipedia" = table_wikipedia
)

# Merge the tables by doi and add new rows if doi doesn't match
merged_table <- combined_table %>%
  group_by(doi) %>%
  summarise(title = first(title),
            journal = first(journal),
            authors1 = first(authors1),
            year = first(year),
            Policies = sum(Policies),
            Twitter = sum(Twitter),
            Facebook = sum(Facebook),
            Blogs = sum(Blogs),
            News = sum(News),
            Wikipedia = sum(Wikipedia)) %>%
  ungroup()

# Display the merged table using DT::datatable
Altimetrics.Table <- DT::datatable(merged_table, rownames = FALSE, width = "100%", 
              options = list(dom = 't', scrollY = '800px', pageLength = 20),
              caption = "Merged Table of Altmetrics for Different Categories")
Altimetrics.Table

# Save the file
write.csv(merged_table, file = here("Plots/AltimetricTable.csv"))
```

Plots:

```{r}
#| fig-width: 8 # Width for figures in inches
#| fig-height: 4 # Height for figures in inches
pos <- position_jitter(width = 0.1, height = 0.1, seed = 1) #used below to add jitter

#Altmetric scores across article publication years:
Altmetrics %>% 
  filter(!is.na(score)) %>% #without articles that have no scores
  filter(!is.na(year)) %>% #without articles that have no year
  ggplot(aes(x = year, y = score)) + 
  geom_point(color = "black", alpha = 0.25, position = pos) +
  xlim(2016.5, 2019.5) + 
  theme_minimal()  +
  ylab("total Altmetric score") +
  xlab("year")

#Altmetric journal context percentile across years as a scatterplot
Altmetrics %>% 
  filter(!is.na(year)) %>% #without articles that have no year
  ggplot(aes(x = year, y = as.numeric(context.journal.pct))) +
  geom_point(alpha = 0.2, position = pos, color = "black") +
  xlim(2016.5, 2019.5) + 
  theme_minimal() +
  ylab("percentile within journal context") +
  xlab("year")

#Altimetric total counts per type of citation
# Convert the 'year' column to numeric in case it's a factor or character
merged_table$year <- as.numeric(merged_table$year)
#replace NA for 0
merged_table <- merged_table %>%
  mutate_at(vars(Policies, Twitter, Facebook, Blogs, News, Wikipedia), replace_na, replace = 0)
# Sum the counts per category for each year
totalALT_per_year <- merged_table %>%
  group_by(year) %>%
  summarise(
    Policies = sum(Policies),
    Twitter = sum(Twitter),
    Facebook = sum(Facebook),
    Blogs = sum(Blogs),
    News = sum(News),
    Wikipedia = sum(Wikipedia)
  ) %>%
  ungroup()

# Scatter Plot of all
melted_table <- melt(merged_table, id.vars = "year", measure.vars = c("Policies", "Twitter", "Facebook", "Blogs", "News", "Wikipedia"))
ggplot(data = melted_table, aes(x = year, y = value, color = variable)) +
  geom_point() +
  labs(x = "Year", y = "Number of Inputs", title = "Altmetrics Inputs per Year",
       subtitle = "Scatter Plot of Altmetrics Inputs per Year for Different Categories") +
  theme_minimal()

 #Scatter Plot of totals
melted_totals <- melt(totalALT_per_year, id.vars = "year", measure.vars = c("Policies", "Twitter", "Facebook", "Blogs", "News", "Wikipedia"))
# Plot the scatter plot with lines connecting the data points
ggplot(data = melted_totals, aes(x = year, y = value, color = variable, group = variable)) +
  geom_point() +
  geom_line() +  # Add this line to connect the dots
  labs(x = "Year", y = "Total Inputs", title = "Total Altmetrics Inputs per Year",
       subtitle = "Scatter Plot with Connecting Lines for Total Altmetrics Inputs per Year by Category") +
  theme_minimal()

#Top ten most influential journals
# Calculate the influence of each journal by summing inputs across all altmetrics categories
influence_per_journal <- merged_table %>%
  group_by(journal) %>%
  summarise(total_influence = sum(Policies, Twitter, Facebook, Blogs, News, Wikipedia)) %>%
  ungroup() %>%
  arrange(desc(total_influence)) # Sort in descending order of influence
# Select the top 10 most influential journals
top_10_journals <- head(influence_per_journal, 10)
# Plot the bar chart for the top 10 most influential journals
ggplot(top_10_journals, aes(x = reorder(journal, total_influence), y = total_influence)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Journal", y = "Total Influence",
       title = "Top 10 Most Influential Journals",
       subtitle = "Bar Chart of Journals by Total Influence",
       caption = "Influence measured by sum of inputs across all altmetrics categories") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
        plot.caption = element_text(hjust = 0))
```

## Part 4. Rayyan screening and joint datasets

This part includes the most relevant results from joining bibliometrics data with the metadata included during the Rayyan screening.

Before starting, I will need to organize the country data

```{r}
#Who is driving research effort
#For the joint sectors set:
# Extract countries from the affiliations
bibmetaSEP_cc <- metaTagExtraction(bibmetaSEP, Field = "AU_CO", sep = ";")
# Separate countries when multiple authors
bibmetaSEP_cc <- bibmetaSEP_cc %>%
  separate_rows(AU_CO, sep = ";")
# Remove leading and trailing white spaces
bibmetaSEP_cc$AU_CO <- str_trim(bibmetaSEP_cc$AU_CO)
# Remove empty strings
bibmetaSEP_cc <- bibmetaSEP_cc %>%
  filter(AU_CO != "")
#remove countries with longitude >180 to make equal projection-like map without artifacts
world_map <- map_data("world") %>% 
  filter(! long > 180
  )
# Format country names to match regions on the world map
bibmetaSEP_cc$region <- str_to_title(bibmetaSEP_cc$AU_CO)
#need to split region column
region <- str_split_fixed(bibmetaSEP_cc$region, ";", 12)
regiondf <- as.data.frame(region)
#split labels into individual countries
region <- str_split_fixed(bibmetaSEP_cc$region, ";", 12)
#put species into a vector
region.vec <- as.vector(region)
#region.vec
#remove the ""
region.vec <- region.vec[region.vec != ""] 
#table(region.vec) #visualise the species frequencies
#convert table to a data frame
region.SEP <- data.frame(table(region.vec),stringsAsFactors = FALSE)
#region.SEP
#rename country variables from region.vec
region.SEP <- rename(region.SEP, region = region.vec)
region.SEP <- rename(region.SEP, n = Freq)
#rename Usa to USA and United Kingdom to UK for consistency
region.SEP$region <- recode(region.SEP$region, 'Usa' = 'USA') 
region.SEP$region <- recode(region.SEP$region, 'United Kingdom' = 'UK') 
#region.SEP

#For the dataset with sectors as rows
bibmetaEXTRA_cc <- metaTagExtraction(bibmetaEXTRA, Field = "AU_CO", sep = ";")
# Separate countries when multiple authors
bibmetaEXTRA_cc <- bibmetaEXTRA_cc %>%
  separate_rows(AU_CO, sep = ";")
# Remove leading and trailing white spaces
bibmetaEXTRA_cc$AU_CO <- str_trim(bibmetaEXTRA_cc$AU_CO)
# Remove empty strings
bibmetaEXTRA_cc <- bibmetaEXTRA_cc %>%
  filter(AU_CO != "")
# Format country names to match regions on the world map
bibmetaEXTRA_cc$region <- str_to_title(bibmetaEXTRA_cc$AU_CO)
#need to split region column
region2 <- str_split_fixed(bibmetaEXTRA_cc$region, ";", 12)
regiondf2 <- as.data.frame(region2)
#split labels into individual countries
region2 <- str_split_fixed(bibmetaEXTRA_cc$region, ";", 12)
#put species into a vector
region.vec2 <- as.vector(region2)
#region.vec2 #to check
#remove the ""
region.vec2 <- region.vec2[region.vec2 != ""] 
#table(region.vec) #visualise the species frequencies
#convert table to a data frame
region.EXTRA <- data.frame(table(region.vec2),stringsAsFactors = FALSE)
#region.EXTRA  #to check
#rename country variables from region.vec
region.EXTRA <- rename(region.EXTRA, region = region.vec2)
region.EXTRA <- rename(region.EXTRA, n = Freq)
#rename Usa to USA and United Kingdom to UK for consistency
region.EXTRA$region <- recode(region.EXTRA$region, 'Usa' = 'USA') 
region.EXTRA$region <- recode(region.EXTRA$region, 'United Kingdom' = 'UK') 
#region.EXTRA


#research effort per country
#pie chart about total % points of view 
# Create a new data frame with regions grouped as "Others" if n < 100
regions_to_keep <- region.SEP %>%
  group_by(region) %>%
  summarize(total_articles = sum(n)) %>%
  filter(total_articles >= 100) %>%
  pull(region)
# Convert regions_to_keep to a character vector
regions_to_keep <- as.character(regions_to_keep)
# Create a new data frame with regions grouped as "Others" if n < 20
grouped_dataset <- region.SEP %>%
  mutate(region = fct_other(region, keep = regions_to_keep)) %>%
  group_by(region) %>%
  summarize(n = sum(n))
# Calculate the total number of articles
total_articles <- sum(grouped_dataset$n)
# Calculate the percentage of articles for each region
grouped_dataset$percentage <- grouped_dataset$n / total_articles
# Create the chart
ggplot(grouped_dataset, aes(x = "", y = n, fill = region)) +
  geom_bar(stat = "identity", width = 1) +
  geom_text(aes(label = percent(percentage)), position = position_stack(vjust = 0.5)) +
  coord_polar("y", start = 0) +
  labs(title = "Pie Chart of Regions", fill = "Regions") +
  theme_minimal() +
  theme(axis.text.x = element_blank(),       # Remove x-axis labels
        axis.ticks.x = element_blank())     # Remove x-axis ticks
```

### 4.1 Shifting perceptions

How perceptions have shifted through years, by sectors and depending on the Tier.

Note: Tier 1 - Studies the Blue Economy as a subject \| Tier 2 - Studies about applications of Blue Economy

For this first part, we will use the data frame were sectors are grouped in columns (metaSEP)

```{r}
#We will see total shifts in point of view in accordance with year (as a percentage of total published data that year), per industry, a combination of both, and shifts in time per country
# Calculate the total percentage for each point of view category
view_counts <- metaSEP %>%
  group_by(View) %>%
  summarise(count = n()) %>%
  mutate(percentage = count / sum(count) * 100)

#pie chart about total % points of view 
view_counts2 <- table(metaSEP$View)
ggplot(data.frame(view_counts2), aes(x = "", y = Freq, fill = names(view_counts2))) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  labs(title = "Distribution of Points of View") +
  theme_void()

# Calculate the percentage for each point of view category within each year
view_percentagesYR <- metaSEP %>%
  group_by(year, View) %>%
  summarise(count = n(), .groups = "drop") %>%  # Specify .groups = "drop" to reset grouping
  group_by(year) %>%
  mutate(percentage = count / sum(count) * 100)

# Calculate the percentage for each point of view category within each year
ggplot(view_percentagesYR, aes(x = factor(year), y = percentage, fill = View)) +
  geom_bar(stat = "identity", position = "fill") +
  labs(x = "Year", y = "Percentage", title = "Shift in Point of View") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom", 
        legend.title = element_blank(),
        panel.background = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank(),
        plot.title = element_text(size = 16, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        legend.text = element_text(size = 10),
        legend.box.background = element_rect(colour = "black", size = 0.5)) +
  scale_fill_brewer(palette = "Set1")

# Now just a chart of total points of view (no percentages)
#first we group the data
view_counts_total_per_year <- metaSEP %>%
  group_by(year, View) %>%
  summarise(count = n(), .groups = "drop") %>%  # Specify .groups = "drop" to reset grouping
  ungroup()

# Stacked bar chart for total points of view per year
ggplot(view_counts_total_per_year, aes(x = factor(year), y = count, fill = View)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(x = "Year", y = "Count", title = "Distribution of Points of View per Year") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom",
        legend.title = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank(),
        plot.title = element_text(size = 16, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        legend.text = element_text(size = 10),
        legend.box.background = element_rect(colour = "black", size = 0.5))
#Statistical test
# Run the linear regression
lm_mode_growth <- lm(count ~ year, data = view_counts_total_per_year)
# Obtain the summary statistics
summary(lm_mode_growth)

# Scatter plot for total points of view per year
ggplot(view_counts_total_per_year, aes(x = factor(year), y = count, color = View, group = View)) +
  geom_point() +
  geom_line() +
  labs(x = "Year", y = "Count", title = "Distribution of Points of View per Year") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom",
        legend.title = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank(),
        plot.title = element_text(size = 16, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        legend.text = element_text(size = 10),
        legend.box.background = element_rect(colour = "black", size = 0.5)) +
  scale_color_brewer(palette = "Set1")
```

Now per Tier (using metaSEP)

```{r}
#First a chart to understand how many articles of each we have
# Calculate the total percentage for each Tier category
tier_counts <- metaSEP %>%
  group_by(Classification) %>%
  summarise(count = n()) %>%
  mutate(percentage = count / sum(count) * 100)
#pie chart about total % points of view 
tier_counts <- table(metaSEP$Classification)
# Calculate the total quantity for each category
total_counts <- as.data.frame(table(metaSEP$Classification))
# Pie chart with captions
ggplot(data.frame(tier_counts), aes(x = "", y = Freq, fill = names(tier_counts))) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  geom_text(data = total_counts, aes(label = Freq), position = position_stack(vjust = 0.5), size = 4) +
  labs(title = "Distribution of Tiers") +
  theme_void()

#growth in articles per year
# Calculate the number of articles per year per tier
articles_per_year_per_tier <- metaSEP %>%
  group_by(Classification, year) %>%
  summarize(NumArticles = n())
# Create the line plot for growth in number of articles per year per tier
ggplot(articles_per_year_per_tier, aes(x = year, y = NumArticles, color = Classification)) +
  geom_line() +
  geom_point() +
  labs(title = "Growth in Number of Articles per Year per Tier",
       x = "Year", y = "Number of Articles",
       color = "Tier") +
  theme_minimal()

#Now the differences
# Filter for Tier1 and calculate the percentage for each point of view category within each year
view_percentagesYR_tier1 <- metaSEP %>%
  filter(Classification == "Tier1") %>%
  group_by(year, View) %>%
  summarise(count = n(), .groups = "drop") %>%  # Specify .groups = "drop" to reset grouping
  group_by(year) %>%
  mutate(percentage = count / sum(count) * 100) %>%
  mutate(Classification = "Tier1")

# Filter for Tier2 and calculate the percentage for each point of view category within each year
view_percentagesYR_tier2 <- metaSEP %>%
  filter(Classification == "Tier2") %>%
  group_by(year, View) %>%
  summarise(count = n(), .groups = "drop") %>%  # Specify .groups = "drop" to reset grouping
  group_by(year) %>%
  mutate(percentage = count / sum(count) * 100) %>%
  mutate(Classification = "Tier2")

# Combine the dataframes
combined_data <- bind_rows(view_percentagesYR_tier1, view_percentagesYR_tier2)
# Create the plot
ggplot(combined_data, aes(x = factor(year), y = percentage, fill = View)) +
  geom_bar(stat = "identity", position = "fill") +
  labs(x = "Year", y = "Percentage",
       title = "Shift in Point of View by Classification",
       fill = "Point of View",
       caption = "Blue Economy Research: Tier 1 | Blue Economy Applications: Tier 2") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom",
        legend.title = element_blank(),
        panel.background = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor = element_blank(),
        plot.title = element_text(size = 16, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        legend.text = element_text(size = 10),
        legend.box.background = element_rect(colour = "black", size = 0.5)) +
  scale_fill_brewer(palette = "Set1") +
  facet_wrap(~ Classification, ncol = 1)
```

If we want to see perception shifts per sector, we need the other dataframe (metaEXTRA)

```{r}
# Calculate the percentage for each point of view category within each sector
view_percentagesSEC <- metaEXTRA %>%
  group_by(Sector, View) %>%
  summarise(count = n()) %>%
  group_by(Sector) %>%
  mutate(percentage = count / sum(count) * 100) %>%
  ungroup()
# Reorder levels of "Sector" based on increasing negative view percentage
view_percentagesSEC <- view_percentagesSEC %>%
  arrange(if_else(View == "negative", percentage, Inf)) %>%
  mutate(Sector = fct_inorder(Sector))
# Reorder levels of "Sector" with "Others" last
view_percentagesSEC$Sector <- fct_relevel(view_percentagesSEC$Sector, "Others", after = Inf)
# Create the plot with facets
ggplot(view_percentagesSEC, aes(x = Sector, y = percentage, fill = View)) +
  geom_bar(stat = "identity", position = "fill") +
  labs(x = "Sector", y = "Percentage", title = "Shift in Point of View by Sector") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom",
        legend.title = element_blank(),
        plot.title = element_text(size = 16, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        legend.text = element_text(size = 10),
        panel.grid.major.y = element_line(color = "gray90"),
        panel.grid.minor = element_blank()) +
  scale_fill_brewer(palette = "Set1")

#Another option
view_countsSEC <- metaEXTRA %>%
  group_by(Sector, View) %>%
  summarise(count = n())
ggplot(view_countsSEC, aes(x = View, y = count, fill = Sector)) +
  geom_bar(stat = "identity") +
  labs(x = "View", y = "Count", title = "Point of View by Sector") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom",
        legend.title = element_blank(),
        plot.title = element_text(size = 16, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        legend.text = element_text(size = 10),
        panel.grid.major.y = element_line(color = "gray90"),
        panel.grid.minor = element_blank()) +
  scale_fill_brewer(palette = "Set1")

# Calculate the percentage for each point of view category within each sector per year
view_percentagesSEC_YR <- metaEXTRA %>%
  group_by(Sector, View, year) %>%
  summarise(count = n()) %>%
  group_by(Sector, year) %>%
  mutate(percentage = count / sum(count) * 100) %>%
  ungroup()
# Reorder levels of "Sector" with "Others" last
view_percentagesSEC_YR$Sector <- fct_relevel(view_percentagesSEC_YR$Sector, "Others", after = Inf)
# Create the plot
ggplot(view_percentagesSEC_YR, aes(x = year, y = percentage, fill = View)) +
  geom_area(position = "stack") +
  facet_wrap(~ Sector, ncol = 2, scales = "free_y") +
  labs(x = "Year", y = "Percentage", title = "Shift in Point of View by Sector and Year") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom",
        legend.title = element_blank(),
        plot.title = element_text(size = 16, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        legend.text = element_text(size = 10),
        panel.grid.major.y = element_line(color = "gray90"),
        panel.grid.minor = element_blank()) +
  scale_fill_brewer(palette = "Set1")

#scope into negativity
# Filter the data for "Negativity" view only
negativity_data <- metaEXTRA %>%
  filter(View == "negative")
# Calculate the count of negativity views for each sector per year
negativity_counts_SEC_YR <- negativity_data %>%
  group_by(Sector, year) %>%
  summarise(negativity_count = n())
# Create the scatter plot with connecting lines
ggplot(negativity_counts_SEC_YR, aes(x = year, y = negativity_count, group = Sector, color = Sector)) +
  geom_point(size = 3) +
  geom_line() +
  labs(x = "Year", y = "Negativity View Count", title = "Number of Negativity Views per Sector and Year") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom",
        legend.title = element_blank(),
        plot.title = element_text(size = 16, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        legend.text = element_text(size = 10),
        panel.grid.major.y = element_line(color = "gray90"),
        panel.grid.minor = element_blank())

#only negative data
# Filter the data to include only "negative" views
negative_dataPER <- view_percentagesSEC_YR %>%
  filter(View == "negative")

# Create the scatter plot for negative field percentages per year for each sector
ggplot(negative_dataPER, aes(x = year, y = percentage, color = Sector, group = Sector)) +
  geom_point() +
  geom_line() +  # Add this line to connect the dots
  labs(x = "Year", y = "Percentage", title = "Negative Field Percentages per Year for Each Sector",
       subtitle = "Scatter Plot with Connecting Lines of Negative Field Percentages by Sector and Year") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom",
        legend.title = element_blank(),
        plot.title = element_text(size = 16, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        legend.text = element_text(size = 10),
        panel.grid.major.y = element_line(color = "gray90"),
        panel.grid.minor = element_blank()) +
  scale_color_brewer(palette = "Set1")
```

If we want to see some altimetrics with this, we will join the merged_table with bibmetaEXTRA and plot

```{r}
#First we will merge the altimetrics table (merged_table) with bibmetaEXTRA
# Rename the "doi" column to "DI" in the merged_table
merged_table2 <- merged_table %>%
  rename(DI = doi)
# Now, merge the two data frames by "DI"
alltogether_EXTRA <- merge(merged_table2, bibmetaEXTRA, by = "DI")

#Now lets see point of view trend by media type
# Melt the data frame to long format for easier plotting
melted_dataVIEWS <- alltogether_EXTRA %>%
  select(DI, View, Policies, Twitter, Facebook, Blogs, News, Wikipedia) %>%
  pivot_longer(cols = c(Policies, Twitter, Facebook, Blogs, News, Wikipedia),
               names_to = "Media", values_to = "Count")
# Plot the trend of points of view by media type
ggplot(data = melted_dataVIEWS, aes(x = Media, y = Count, fill = View, group = View)) +
  geom_col(position = "dodge") +
  labs(x = "Media Type", y = "Count", title = "Points of View Trend by Media Type",
       fill = "View") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Shifting trends in top countries

```{r}
# Group by Country and Sector, and count the number of articles in each sector for each country
view_counts_by_country <- bibmetaEXTRA_cc %>%
  group_by(AU_CO, View) %>%
  summarise(article_count = n()) %>%
  ungroup()
# Get the top countries based on the total number of articles published
top_countriesVIEWS <- view_counts_by_country %>%
  group_by(AU_CO) %>%
  summarise(total_articles = sum(article_count)) %>%
  top_n(10) %>%
  pull(AU_CO)
# Filter the data to keep only the top countries
view_counts_top_countries <- view_counts_by_country %>%
  filter(AU_CO %in% top_countriesVIEWS)
# Summarize the count of articles in each sector for each country
view_counts_summarized.CC <- view_counts_top_countries %>%
  group_by(AU_CO, View) %>%
  summarise(total_articles = sum(article_count)) %>%
  ungroup()
# Sort the data by country and total_articles in descending order
view_counts_summarized.CC <- view_counts_summarized.CC %>%
  arrange(desc(total_articles))
# Create the bar plot
ggplot(view_counts_summarized.CC, aes(x = reorder(AU_CO, total_articles), y = total_articles, fill = View)) +
  geom_bar(stat = "identity") +
  labs(x = "Country", y = "Total Articles", title = "Top Sectors Published by Top Countries",
       fill = "View") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "right",
        plot.title = element_text(size = 16, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        axis.line.x = element_line(color = "black"),
        panel.grid.major.y = element_line(color = "gray90"),
        panel.grid.minor = element_blank()) +
  scale_fill_brewer(palette = "Set1")

#now as a percentage
# Calculate the total articles for each country
total_articles_by_country <- view_counts_summarized.CC %>%
  group_by(AU_CO) %>%
  summarise(total_articles = sum(total_articles)) %>%
  ungroup()
# Join the total_articles_by_country data to view_counts_summarized
view_counts_summarized.CC <- view_counts_summarized.CC %>%
  left_join(total_articles_by_country, by = "AU_CO")
# Calculate the percentage of total articles for each country and view combination
view_counts_summarized.CC <- view_counts_summarized.CC %>%
  mutate(percentage_total_articles = (total_articles.x / total_articles.y) * 100) %>%
  select(-total_articles.y)
# Create the bar plot with percentage instead of the count
ggplot(view_counts_summarized.CC, aes(x = reorder(AU_CO, -percentage_total_articles), y = percentage_total_articles, fill = View)) +
  geom_bar(stat = "identity") +
  labs(x = "Country", y = "Percentage of Total Articles", title = "Top Sectors Published by Top Countries",
       fill = "View") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "right",
        plot.title = element_text(size = 16, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        axis.line.x = element_line(color = "black"),
        panel.grid.major.y = element_line(color = "gray90"),
        panel.grid.minor = element_blank()) +
  scale_fill_brewer(palette = "Set1")

#Now as a percentage of the total global number
# Calculate the total number of articles across all countries
total_articles_global <- sum(view_counts_summarized.CC$total_articles.x)
# Calculate the percentage of total articles for each country and view combination
view_counts_summarized2 <- view_counts_summarized.CC %>%
  mutate(percentage_total_articles = (total_articles.x / total_articles_global) * 100)
# Create the bar plot with percentage instead of the count
ggplot(view_counts_summarized2, aes(x = reorder(AU_CO, -percentage_total_articles), y = percentage_total_articles, fill = View)) +
  geom_bar(stat = "identity") +
  labs(x = "Country", y = "Percentage of Total Articles", title = "Top Sectors Published by Top Countries (Percentages of Total)",
       fill = "View") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "right",
        plot.title = element_text(size = 16, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        axis.line.x = element_line(color = "black"),
        panel.grid.major.y = element_line(color = "gray90"),
        panel.grid.minor = element_blank()) +
  scale_fill_brewer(palette = "Set1")

#pie chart of negative contributions
# Filter the data to include only rows with negative views
negative_views_data <- view_counts_summarized2 %>%
  filter(View == "negative")
# Create the pie chart
ggplot(negative_views_data, aes(x = "", y = percentage_total_articles, fill = AU_CO)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  labs(title = "Percentage of Articles with Negative Views by Country") +
  theme_void() +
  theme(legend.position = "bottom") 
```

### 4.2 Sector effort and influence

Here we will use the dataset with sectors in extra rows (ALLDATA_EXTRA) and (bibmetaEXTRA).

First we want to see the total effort

```{r}
#Percentages of articles per sector (without considering point of view)
# Calculate the percentage for each sector
sector_counts <- metaEXTRA %>%
  group_by(Sector) %>%
  summarise(count = n()) %>%
  mutate(percentage = count / sum(count) * 100)

#pie chart about total % points of view 
sector_counts <- table(metaEXTRA$Sector)
ggplot(data.frame(sector_counts), aes(x = "", y = Freq, fill = names(sector_counts))) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  labs(title = "Distribution of Sector Target") +
  theme_void()

#I want to remove the non-targeted articles:
# Calculate the percentage for each sector
sector_counts <- metaEXTRA %>%
  group_by(Sector) %>%
  summarise(count = n()) %>%
  mutate(percentage = count / sum(count) * 100)

# Filter out the 'non-targeted' sector
sector_counts <- sector_counts %>% 
  filter(Sector != "non-targeted")

# Create the pie chart
ggplot(data = sector_counts, aes(x = "", y = count, fill = Sector)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  labs(title = "Distribution of Sector Target (non-targeted excluded)") +
  theme_void()

#And make one of all these vs non targeted, including % numbers:
# Step 1: Update the 'Sector' column
metaSEP2 <- metaSEP %>%
  mutate(Sector = ifelse(Sector == "non-targeted", "non-targeted", "sector-related"))

# Step 2: Calculate the percentage for "non-targeted" and "Sector-related" categories
sector_counts2 <- metaSEP2 %>%
  group_by(Sector) %>%
  summarise(count = n()) %>%
  mutate(percentage = count / sum(count) * 100)

# Step 3: Create the bar plot
ggplot(sector_counts2, aes(x = Sector, y = count, fill = Sector)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(percentage, 2), "%")), position = position_stack(vjust = 0.5)) +
  labs(x = "", y = "Count", title = "Comparison of non-targeted and Sector-related") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none",
        plot.title = element_text(size = 16, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        axis.text.y = element_text(margin = margin(r = 10)),
        axis.line.x = element_line(color = "black"),
        panel.grid.major.y = element_line(color = "gray90"),
        panel.grid.minor = element_blank()) +
  scale_fill_brewer(palette = "Set1")

#pie chart view
ggplot(sector_counts2, aes(x = "", y = count, fill = Sector)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +  # Convert the bar chart to a pie chart
  geom_text(aes(label = paste0(round(percentage, 2), "%")), 
            position = position_stack(vjust = 0.5),
            size = 4,    # Adjust the font size of the percentage labels
            vjust = -0.3,   # Adjust the vertical position of the labels
            fontface = "bold") +   # Set the font face to bold
  labs(x = "", y = "Count", title = "Comparison of non-targeted and Sector-related") +
  theme_minimal() +
  theme(axis.text.x = element_blank(),  # Remove x-axis labels (empty string)
        axis.text.y = element_text(margin = margin(r = 10)),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        axis.line.x = element_line(color = "black"),
        plot.title = element_text(size = 16, face = "bold"),
        legend.position = "bottom",  # Move the legend to the bottom
        legend.title = element_blank(),
        legend.text = element_text(size = 10),
        panel.grid.major.y = element_line(color = "gray90"),
        panel.grid.minor = element_blank()) +
  scale_fill_brewer(palette = "Set1")

#One last chart trying to summarize
# Calculate the total count
total_count <- sum(view_countsSEC$count)
# Create the pie chart with total count
ggplot(view_countsSEC, aes(x = "", y = count, fill = Sector)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  labs(x = NULL, y = NULL, title = "Total Counts per Sector") +
  theme_minimal() +
  theme(
    axis.line = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    plot.title = element_text(size = 16, face = "bold"),
    legend.position = "bottom",
    legend.title = element_blank(),
    panel.grid = element_blank(),
    panel.background = element_blank()
  ) +
  scale_fill_brewer(palette = "Set1") +
  geom_text(aes(label = count), position = position_stack(vjust = 0.5), size = 1.5) +
  annotate(
    "text",
    x = 0, y = 0,
    label = paste("Total Count:", total_count),
    size = 2,
    color = "black"
  )
```

Research effort by country/sector

```{r}
#research effort by country/sector
# Group by Country and Sector, and count the number of articles in each sector for each country
sector_counts_by_country <- bibmetaEXTRA_cc %>%
  group_by(AU_CO, Sector) %>%
  summarise(article_count = n()) %>%
  ungroup()
# Get the top countries based on the total number of articles published
top_countries <- sector_counts_by_country %>%
  group_by(AU_CO) %>%
  summarise(total_articles = sum(article_count)) %>%
  top_n(10) %>%
  pull(AU_CO)
# Filter the data to keep only the top countries
sector_counts_top_countries <- sector_counts_by_country %>%
  filter(AU_CO %in% top_countries)
# Summarize the count of articles in each sector for each country
sector_counts_summarized <- sector_counts_top_countries %>%
  group_by(AU_CO, Sector) %>%
  summarise(total_articles = sum(article_count)) %>%
  ungroup()
# Sort the data by country and total_articles in descending order
sector_counts_summarized <- sector_counts_summarized %>%
  arrange(desc(total_articles))
# Create the bar plot
ggplot(sector_counts_summarized, aes(x = reorder(AU_CO, total_articles), y = total_articles, fill = Sector)) +
  geom_bar(stat = "identity") +
  labs(x = "Country", y = "Total Articles", title = "Top Sectors Published by Top Countries",
       fill = "Sector") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "right",
        plot.title = element_text(size = 16, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        axis.line.x = element_line(color = "black"),
        panel.grid.major.y = element_line(color = "gray90"),
        panel.grid.minor = element_blank()) +
  scale_fill_brewer(palette = "Set1")

```

Industry clusters for the top 10 publishing countries

```{r}
# Get the top countries based on the total number of articles published
top_countries <- sector_counts_by_country %>%
  group_by(AU_CO) %>%
  summarise(total_articles = sum(article_count)) %>%
  top_n(10) %>%
  pull(AU_CO)

# Filter the data to keep only the top countries
sector_counts_top_countries <- sector_counts_by_country %>%
  filter(AU_CO %in% top_countries)

# Create a cross-tabulation of sectors by countries
sector_counts_matrix <- sector_counts_top_countries %>%
  pivot_wider(names_from = Sector, values_from = article_count, values_fill = 0)
#str(sector_counts_matrix)
# Calculate the distance between countries based on sectors (Euclidean distance)
distance_matrix <- as.matrix(dist(t(sector_counts_matrix[,-1])))

# Perform hierarchical clustering on the distance matrix using the dendextend package
dend <- as.dendrogram(hclust(as.dist(distance_matrix)))

# Reorder the rows of the data based on the clustering
reordered_matrix <- sector_counts_matrix

# Extract the row names (countries) and convert them to a factor
reordered_countries <- factor(rownames(reordered_matrix), levels = rownames(reordered_matrix))

# Create the cluster map
ggplot(data = gather(reordered_matrix, Sector, Count, -AU_CO)) +
  aes(x = AU_CO, y = Sector) +
  geom_tile(aes(fill = Count), color = "white") +
  scale_fill_viridis_c(option = "magma", direction = -1) +  # Change the color palette if desired
  labs(x = "Country", y = "Sector", title = "Cluster Map of Sectors Focus by Top Publishing Countries") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(hjust = 0.5),
        legend.position = "right",
        plot.title = element_text(size = 16, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        axis.line.x = element_line(color = "black"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
ggsave(here("Plots/clustermap.jpg"), width = 20, height = 20, units = "cm") #save the map on the plot folder
```

If we want to see some altimetrics with this, we will use the joint set (alltogether_EXTRA)

```{r}
#Top publishing sectors per media type (which sectors have most attention)
# Melt the data frame to long format for easier plotting
melted_dataSECTORS <- alltogether_EXTRA %>%
  select(DI, Sector, Policies, Twitter, Facebook, Blogs, News, Wikipedia) %>%
  pivot_longer(cols = c(Policies, Twitter, Facebook, Blogs, News, Wikipedia),
               names_to = "Media", values_to = "Count")
# Plot the trend of points of view by media type
ggplot(data = melted_dataSECTORS, aes(x = Media, y = Count, fill = Sector, group = Sector)) +
  geom_col(position = "dodge") +
  labs(x = "Sector", y = "Count", title = "Sector Trend by Media Type",
       fill = "Sector") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Filter out the 'non-targeted' sector
melted_dataSECTORSNF <- melted_dataSECTORS %>%
  filter(Sector != "non-targeted")
# Plot the trend of points of view by media type
ggplot(data = melted_dataSECTORSNF, aes(x = Media, y = Count, fill = Sector, group = Sector)) +
  geom_col(position = "dodge") +
  labs(x = "Media", y = "Count", title = "Sector Trend by Media Type (without non-targeted)",
       fill = "Sector") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
